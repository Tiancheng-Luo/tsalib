{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from tsalib import dim_vars, get_dim_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Principles\n",
    "**Dimension Variables** (DVs) are the core abstractions behind tsalib. \n",
    "- They allow specifying and modifying shapes of tensors *symbolically*, i.e., using named symbols corresponding to different dimensions of tensor. \n",
    "- Making dimension names explicit enables cleaner, DRY code, symbolic shape assertions, and faster debugging.\n",
    "- **Symbolic** shapes or **annotations** are *tuples* over DVs and arithmetic expressions over DVs.\n",
    "\n",
    "The `tsalib` provides a collection of powerful APIs to handle all kinds of shape transformations using explicit dimension variables and shape annotations.  \n",
    "\n",
    "\n",
    "- Designed to stay light, easy to incorporate into existing workflow with minimal code changes.\n",
    "- The API includes both library-independent and dependent parts, giving developers flexibility in how they choose to incorporate `tsalib` in their workflow.\n",
    "- Avoid deeper integration into popular tensor libraries to keep `tsalib` light-weight and avoid backend-inflicted bugs.\n",
    "\n",
    "Some popular models (resnet, transformer) annotated/re-written with tsalib can be found in the [models](models/) directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare dimension variables\n",
    "Dimension variables model both the `name` and the default `size` of a tensor.   \n",
    "Format: **name(symbol):size**   --  `symbol` and `size` are optional\n",
    "\n",
    "We can declare dimension variables **globally** (Dimensions used in programs are known upfront and programs don't modify dimension names).  \n",
    "Even better, we can put all these definitions in the Config dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals variables prefixed with underscores\n",
    "_B, _T, _D, _K = dim_vars('Batch(b):20 SeqLength(t):10 EmbeddingDim(d):100 K(k):1')\n",
    "_C, _H, _W = dim_vars('Channels(c):3 Height(h):256 Width(w):256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decls():\n",
    "    print('\\nTest declarations ..')\n",
    "    #local declarations\n",
    "    print(f'B, C, D = {_B}, {_C}, {_D}')\n",
    "\n",
    "    #strict=False allows overwriting previous declarations\n",
    "    H, W = dim_vars ('Height(h):256 Width(w):256', exists_ok=True) \n",
    "    print(f'H, W = {H}, {W}')\n",
    "\n",
    "# Supports arithmetic over a combination of dim vars and other Python variables\n",
    "def test_arith():\n",
    "    print('\\nTest arithmetic ..')\n",
    "    _K, _W, _B, _H = get_dim_vars('k w b h') \n",
    "    _K = _W * 2\n",
    "    h = 4\n",
    "    print((h, _H // h, _K, _B*2))\n",
    "\n",
    "# Use dimension variables in lieu of constant size values\n",
    "# note: dim_var declaration must include size of the variable\n",
    "def test_cast_int():\n",
    "    print('\\nTest integer cast ..')\n",
    "    B, C = get_dim_vars('b c')\n",
    "    x = np.zeros((B, C))\n",
    "    print(f'shape of array: ({B},{C}): {x.shape}')\n",
    "    return x\n",
    "    \n",
    "def basic_tests():\n",
    "    test_decls()\n",
    "    test_arith()\n",
    "    x = test_cast_int()\n",
    "    # Test assertions over symbolic shapes\n",
    "    assert x.shape == (_B,_C)\n",
    "    print ('assertions hold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test declarations ..\n",
      "B, C, D = Batch(b):20, Channels(c):3, EmbeddingDim(d):100\n",
      "H, W = Height(h):256, Width(w):256\n",
      "\n",
      "Test arithmetic ..\n",
      "(4, floor(Height(h)/4):64, 2*Width(w):512, 2*Batch(b):40)\n",
      "\n",
      "Test integer cast ..\n",
      "shape of array: (Batch(b):20,Channels(c):3): (20, 3)\n",
      "assertions hold\n"
     ]
    }
   ],
   "source": [
    "basic_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tsalib usage\n",
    "Can be used to manage tensor shapes with **arbitrary** tensor libraries. Here, examples with *numpy* and *pytorch*.\n",
    "- Create new tensors (independent of actual dimension sizes)\n",
    "- **Annotate** tensor variables (widely considered best practice, otherwise done using adhoc comments)\n",
    "- Check symbolic **assertions** (assertions **do not** change even if dimension size changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test usage with numpy ..\n",
      "original array: (Batch(b):20, EmbeddingDim(d):100): (20, 100)\n",
      "after stack: (2, Batch(b):20, EmbeddingDim(d):100): (2, 20, 100)\n",
      "after mean along axis = 1: (2, EmbeddingDim(d):100): (2, 100)\n"
     ]
    }
   ],
   "source": [
    "def test_numpy():\n",
    "    print('\\nTest usage with numpy ..')\n",
    "    B, D = get_dim_vars('b d')\n",
    "    import numpy as np\n",
    "    a: (B, D) = np.zeros((B,D))\n",
    "    print(f'original array: {(B,D)}: {a.shape}')\n",
    "\n",
    "    b: (2, B, D) = np.stack([a, a])\n",
    "    print(f'after stack: {(2,B,D)}: {b.shape}')\n",
    "\n",
    "    ax = (2,B,D).index(B)\n",
    "    c: (2, D) = np.mean(b, axis=ax)\n",
    "    print(f'after mean along axis = {ax}: {(2,D)}: {c.shape}')\n",
    "\n",
    "test_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test usage with pytorch ..\n",
      "Asserting b.size() == (2,B,D)\n",
      "Assertion on c.size()\n"
     ]
    }
   ],
   "source": [
    "def test_pytorch():\n",
    "    print('\\nTest usage with pytorch ..')\n",
    "    B, D = get_dim_vars('b d')\n",
    "    B, D = dim_vars('Batch:2 EmbedDim:3', exists_ok=True)\n",
    "    import torch\n",
    "\n",
    "    a = torch.Tensor([[1., 2., 4.], [3., 6., 9.]])\n",
    "    assert a.size() == (B, D)\n",
    "\n",
    "    b = torch.stack([a, a])\n",
    "\n",
    "    print ('Asserting b.size() == (2,B,D)')\n",
    "    assert b.size() == (2, B, D)\n",
    "\n",
    "    c = torch.cat([a, a], dim=1)\n",
    "    print ('Assertion on c.size()')\n",
    "    assert c.size() == (B, D*2)\n",
    "\n",
    "test_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Transformations with Dimensions Variables\n",
    "To shape transform without `tsalib`, you either \n",
    "-  **hard-code** integer constants for each dimension's position in shape transformations, or\n",
    "- do shape tuple **surgeries** to compute the 'right' shape (for the general case)\n",
    "\n",
    "Instead, with `tsalib`, use dimension variables or the shorthand symbols directly. \n",
    "\n",
    "`tsalib` provides API for common shape transformations: **view** (reshape), **permute** (transpose) and **expand** (tile).  \n",
    "These are *library-independent*, e.g., shorthand transformation -> target shape tuple -> reshape.\n",
    "\n",
    "One transformation to rule them all : **warp**. Do a sequence of transformations on a tensor.  \n",
    "`warp` is implementated for several popular backend libraries.\n",
    "\n",
    "## Work with Shorthand Shape Notation \n",
    "Writing tuples of shape annotations can get cumbersome.\n",
    "\n",
    "So, instead of (B, T, D), write 'btd' (each dim gets a single char, concatenated together)\n",
    "\n",
    "Instead of (B \\* T, D // 2, T), write 'b * t, d//2, t' (arbitrary arithmetic expressions, comma-separated)\n",
    "\n",
    "Anonymous dimension variables : 'b,,d' omits naming dimension t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshapes (view transformations) using dimension variables\n",
    "These are library independent: `vt` returns target tensor shapes from shorthand transformation spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_reshape: all assertions hold\n",
      "test_reshape_short: all assertions hold\n"
     ]
    }
   ],
   "source": [
    "# without tsalib, this is how we used to do it. See code from BERT.\n",
    "def test_reshape_old ():\n",
    "    x = np.ones((20, 10, 100))\n",
    "    h = 4\n",
    "    new_shape = x.shape[:2] + (h, x.shape[2]//h) #shape surgery\n",
    "    x = x.reshape(new_shape)\n",
    "    print (x.shape)\n",
    "\n",
    "from tsalib import view_transform as vt    \n",
    "    \n",
    "# with tsalib, simply use dimension vars in-place\n",
    "def test_reshape():\n",
    "    B, T, D = get_dim_vars('b t d')\n",
    "    x: (B,T,D) = np.ones((B, T, D))\n",
    "    h = 4\n",
    "    x: (B,T,h,D//h) = x.reshape((B, T, h, D//h))\n",
    "    assert x.shape == (B,T,h,D//h)\n",
    "    print ('test_reshape: all assertions hold')\n",
    "\n",
    "#using shorthand notation, omit dimensions not involved in transformation\n",
    "def test_reshape_short():\n",
    "    B, T, D = get_dim_vars('b t d')\n",
    "    x: (B,T,D) = np.ones((B, T, D))\n",
    "    h = 4\n",
    "    x = x.reshape(vt(f'btd -> b,t,{h},d//{h}', x.shape))\n",
    "    assert x.shape == (B, T, h, D//h)\n",
    "\n",
    "    x1 = x.reshape(vt('b,t,4,k -> b*t,4,k', x.shape))\n",
    "    assert x1.shape == (B*T, h, D//h)\n",
    "    \n",
    "    x1 = x.reshape(vt('b,t,, -> b*t,,', x.shape))\n",
    "    assert x1.shape == (B*T, h, D//h)\n",
    "\n",
    "\n",
    "    print ('test_reshape_short: all assertions hold')\n",
    "\n",
    "\n",
    "#test_reshape_old()\n",
    "test_reshape()\n",
    "test_reshape_short()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose/Permute transformations using dimension variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_permute: all assertions hold\n",
      "test_permute_short: all assertions hold\n"
     ]
    }
   ],
   "source": [
    "from tsalib import  permute_transform as pt\n",
    "from tsalib.transforms import _permute_transform as _pt\n",
    "\n",
    "# permute using dimension variables (internal, recommended to be not used)\n",
    "def test_permute():\n",
    "    B, T, D, K = get_dim_vars('b t d k')\n",
    "    x: (B,T,D,K) = np.ones((B, T, D, K))\n",
    "    perm_indices = _pt(src=(B,T,D,K), to=(D,T,B,K))\n",
    "    assert perm_indices == (2,1,0,3)\n",
    "    x = x.transpose(perm_indices)\n",
    "    assert x.shape == (D,T,B,K)\n",
    "    print ('test_permute: all assertions hold')\n",
    "\n",
    "# shorthand permutes are snazzier (use '_' or ',' as placeholders)\n",
    "def test_permute_short():\n",
    "    B, T, D, K, C, H, W = get_dim_vars('b t d k c h w')\n",
    "    x: (B,T,D,K) = np.ones((B, T, D, K))  \n",
    "    x = x.transpose(pt('btdk -> dtbk')) # (B, T, D, K) -> (D, T, B, K)\n",
    "    assert x.shape == (D,T,B,K)\n",
    "\n",
    "    x = x.transpose(pt('d_b_ -> b_d_')) # (D,T,B,K) -> (B, T, D, K)\n",
    "    assert x.shape == (B,T,D,K)\n",
    "\n",
    "    x: (B, C, H, W) = np.ones((B, C, H, W))\n",
    "    x1 = x.transpose(pt(',c,, -> ,,,c'))\n",
    "    assert x1.shape == (B, H, W, C)\n",
    "    print ('test_permute_short: all assertions hold')\n",
    "test_permute()\n",
    "test_permute_short()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_expand: all assertions hold\n",
      "test_expand_short: all assertions hold\n"
     ]
    }
   ],
   "source": [
    "from tsalib import _expand_transform as et\n",
    "def test_expand():\n",
    "    B, T, D, K = get_dim_vars('b t d k')\n",
    "    \n",
    "    x: (B, T, D) = np.ones((B, T, D))\n",
    "    x: (B, K, T, D) = x[:, None]\n",
    "\n",
    "    expand_shape = et(src=(B,K,T,D), expansions=[(K, K*5)], in_shape=x.shape) #(B, K, T, D) -> (B, K*5, T, D)\n",
    "    assert expand_shape == (-1,5,-1,-1)\n",
    "    print ('test_expand: all assertions hold')\n",
    "\n",
    "def test_expand_short():\n",
    "    B, T, D, K = get_dim_vars('b t d k')\n",
    "    \n",
    "    x: 'btd' = np.ones((B, T, D))\n",
    "    x: 'bktd' = x[:, None]\n",
    "    expand_shape = et(src=(B,K,T,D), expansions='k->k*5', in_shape=x.shape)\n",
    "    assert expand_shape == (-1,5,-1,-1)\n",
    "    print ('test_expand_short: all assertions hold')\n",
    "test_expand()\n",
    "test_expand_short()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *warp* : generalized shape transformations\n",
    "\n",
    "Writing a sequence of shape transformations in code can get cumbersome.  \n",
    "`warp` enables specifying a sequence of transformations together **inline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** processing transform.. v\n",
      " (Batch(b):20, SeqLength(t):10, EmbeddingDim(d):100) -> (Batch(b):20, SeqLength(t):10, 4:4, floor(EmbeddingDim(d)/4):25)\n",
      "   after transform shape is: (20, 10, 4, 25)\n",
      "*** processing transform.. v\n",
      " (Batch(b):20, SeqLength(t):10, 4:4, floor(EmbeddingDim(d)/4):25) -> (Batch(b)*SeqLength(t):200, 4:4, floor(EmbeddingDim(d)/4):25)\n",
      "   after transform shape is: (200, 4, 25)\n",
      "*** processing transform.. v\n",
      " __d -> ,,4,d//4\n",
      "   after transform shape is: (20, 10, 4, 25)\n",
      "*** processing transform.. v\n",
      " b,t,, -> b*t,,\n",
      "   after transform shape is: (200, 4, 25)\n",
      "*** processing transform.. v\n",
      " b*t,, -> b,t,,\n",
      "   after transform shape is: (20, 10, 4, 25)\n",
      "*** processing transform.. v\n",
      " ,,4,d//4 -> ,,d\n",
      "   after transform shape is: (20, 10, 100)\n",
      "test_warp: all assertions hold\n",
      "test_warp_pytorch: all assertions hold\n"
     ]
    }
   ],
   "source": [
    "from tsalib import warp\n",
    "def test_warp():\n",
    "    B, T, D = get_dim_vars('b t d')\n",
    "    \n",
    "    x: 'btd' = np.ones((B, T, D))\n",
    "    \n",
    "    # two view transformations (reshapes) in sequence\n",
    "    x1 = warp(x, 'btd -> b,t,4,d//4 -> b*t,4,d//4', 'vv', debug=True)\n",
    "    assert(x1.shape == (B*T,4,D//4))\n",
    "\n",
    "    # four reshapes in sequence\n",
    "    x2 = warp(x, 'btd -> b,t,4,d//4 -> b*t,4,d//4 -> b,t,4,d//4 -> btd', 'vvvv', debug=False)\n",
    "    assert(x2.shape == (B,T,D))\n",
    "    \n",
    "    # Same reshape sequence in shorthand, specified as list of transformations\n",
    "    x2 = warp(x, ['__d -> ,,4,d//4', 'b,t,, -> b*t,,', 'b*t,, -> b,t,,', ',,4,d//4 -> ,,d'], 'vvvv', debug=True)\n",
    "    assert(x2.shape == (B,T,D))\n",
    "    \n",
    "    print ('test_warp: all assertions hold')\n",
    "    \n",
    "\n",
    "def test_warp_pytorch():\n",
    "    B, T, D = get_dim_vars('b t d')\n",
    "    \n",
    "    import torch\n",
    "    y: 'btd' = torch.randn(B, T, D)\n",
    "    #a reshape followed by permute\n",
    "    y = warp(y, 'btd -> b,t,4,d//4 -> b,4,t,d//4', 'vp', debug=False)\n",
    "    assert(y.shape == (B,4,T,D//4))\n",
    "\n",
    "    print ('test_warp_pytorch: all assertions hold')\n",
    "    \n",
    "test_warp()\n",
    "test_warp_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join: unified stack/concatenate for a list of tensors\n",
    "Crisp shorthand : `'(b,t,d)* -> b,3*t,d'` (**concat**) or `'(b,t,d)* -> b,^,t,d'` (**stack**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_join: all assertions passed\n",
      "test_join_transform: all assertions passed\n"
     ]
    }
   ],
   "source": [
    "from tsalib import join, join_transform\n",
    "def test_join ():\n",
    "    B, T, D = get_dim_vars('b t d')\n",
    "    x1: 'btd' = np.ones((B, T, D))\n",
    "    x2: 'btd' = np.ones((B, T, D))\n",
    "    x3: 'btd' = np.ones((B, T, D))\n",
    "    \n",
    "    #concatenate along the (T) dimension: (b,t,d)* -> (b,3*t,d)\n",
    "    x = join([x1, x2, x3], dims=',*,') \n",
    "    assert x.shape == (B, 3*T, D)\n",
    "\n",
    "    \n",
    "    #stack: join by adding a new dimension to the front: (b,t,d)* -> (^,b,t,d)\n",
    "    x = join([x1, x2, x3], dims='^') \n",
    "    assert x.shape == (3, B, T, D)\n",
    "    \n",
    "    #stack by adding a new dimension at second position: (b,t,d)* -> b,^,t,d)\n",
    "    x = join([x1, x2, x3], dims=',^') \n",
    "    assert x.shape == (B, 3, T, D)\n",
    "    print ('test_join: all assertions passed')\n",
    "    \n",
    "    \n",
    "def test_join_transform():\n",
    "    B, T, D = get_dim_vars('b t d')\n",
    "    x1: 'btd' = np.ones((B, T, D))\n",
    "    x2: 'btd' = np.ones((B, T, D))\n",
    "    x3: 'btd' = np.ones((B, T, D))\n",
    "    \n",
    "    dims = join_transform([x1,x2,x3], '(b,t,d)* -> b,3*t,d')\n",
    "    assert dims == ',*,'\n",
    "    #now use backend-dependent join\n",
    "    \n",
    "    dims = join_transform([x1,x2,x3], '(b,t,d)* -> b,^,t,d')\n",
    "    assert dims == ',^,,'\n",
    "    #now use backend-dependent join\n",
    "    \n",
    "    print ('test_join_transform: all assertions passed')\n",
    "    \n",
    "test_join()\n",
    "test_join_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align one tensor to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test align: all assertion passed\n"
     ]
    }
   ],
   "source": [
    "from tsalib import alignto\n",
    "def test_align():\n",
    "    B, T, D = dim_vars('Batch(b):20 SeqLength(t):10 EmbeddingDim(d):100', exists_ok=True)\n",
    "    \n",
    "    x1 = np.random.randn(D,D)\n",
    "    x2 = np.random.randn(B,D,T,D)\n",
    "\n",
    "    x1_aligned = alignto( (x1, 'dd'), 'bdtd')\n",
    "    assert x1_aligned.shape == (1,D,1,D)\n",
    "    print ('test align: all assertion passed')\n",
    "test_align()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product of two tensors (sharing exactly one dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dot: all assertions passed\n"
     ]
    }
   ],
   "source": [
    "from tsalib import dot\n",
    "import torch\n",
    "def test_dot():\n",
    "    B, C, T, D = get_dim_vars('b c t d')\n",
    "    #x = np.random.rand(B, C, T)\n",
    "    #y = np.random.rand(C, D)\n",
    "    x = torch.randn(B, C, T)\n",
    "    y = torch.randn(C, D)\n",
    "    z = dot('_c_.c_', x, y)\n",
    "    assert z.shape == (B, T, D)\n",
    "    print('test_dot: all assertions passed')\n",
    "test_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce ops (min, max, mean, ..) with tsalib\n",
    "Reduction operators aggregate values over one or more tensor dimensions.  \n",
    "`tsalib` provides `reduce_dims` to compute dimension ids using shorthand notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_reduce: all assertions hold\n"
     ]
    }
   ],
   "source": [
    "from tsalib import reduce_dims as rd\n",
    "\n",
    "def test_reduce ():\n",
    "    assert rd('2bd->2d') == (1,)\n",
    "    assert rd('2bd->2') == (1,2)\n",
    "    print ('test_reduce: all assertions hold')\n",
    "test_reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x: 'btd' = np.random.rand(_B, _T, _D)\n",
    "y = np.mean(x, axis=rd('btd->b'))\n",
    "assert y.shape == (_B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
